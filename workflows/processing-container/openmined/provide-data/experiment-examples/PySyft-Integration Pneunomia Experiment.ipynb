{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Experiment - Kaapana PySyft Integration\n",
    "\n",
    "- Data: Pneunomia (chest x-rays)\n",
    "\n",
    "- Computing plan: Sequential Training\n",
    "\n",
    "Each node is running on an individual Kaapana instance and hosts the locally available data. This notebook and the PySyft-Grid are running on the central Kaapana instance.\n",
    "\n",
    "Please note, this notebook is a simplified version - things like logging where removed for improved readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySyft Imports\n",
    "import syft as sy\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "from utils.datasets import OpenminedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameter & model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training\n",
    "\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.epochs = 40 \n",
    "        self.batch_size = 8\n",
    "        self.lr = 10e-4\n",
    "        self.optimizer = 'SGD'\n",
    "        self.log_interval = 30\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (ResNet18) # TODO!!!\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, channels=3, num_classes=2):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.feature_extractor = models.resnet18(pretrained=False)\n",
    "        self.feature_extractor.load_state_dict(torch.load('resnet18_pretrained_t140/resnet18_pretrained.pt'))\n",
    "        \n",
    "        num_ftrs = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = nn.Linear(num_ftrs, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Network\n",
    "\n",
    "Make sure you started a Grid on central instance (same instance your notebook is running on). Furhtermore, it might be necessary to clean the PySyft-Grid's database before continuing. You can use the Adminer extension to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "GRID_ADDRESS, GRID_PORT = '10.128.129.76', '7000' \n",
    "grid = PublicGridNetwork(hook,\"http://\" + GRID_ADDRESS + \":\" + GRID_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start DAGs on remote machines\n",
    "\n",
    "Call the APIs of the remote instances to start their Data-Providing-Dags. You have to wait a short time until they provide their data - then you should be able so find it by searching for the given experiment-tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values\n",
    "DATASET = 'xray'\n",
    "EXP_TAG = '#xray-exp'\n",
    "DATA_DIR= 'XRAY-split'\n",
    "GRID_NETWORK_URL = 'http://10.128.129.76:7000'\n",
    "\n",
    "\n",
    "participants = [\n",
    "    ('10.128.129.41', 'hd'),\n",
    "    ('10.128.129.6', 'co'),\n",
    "    ('10.128.130.197', 'mu')\n",
    "]\n",
    "\n",
    "# trigger data-provider dags on remote machines\n",
    "for machine, identifier in participants:\n",
    "    json_data = {\n",
    "        'rest_call': {\n",
    "            'global': {\n",
    "                'hostname': machine,\n",
    "                'action_operator_dirs':[DATA_DIR],\n",
    "                'release_name': f'openmined-node-{identifier}'\n",
    "            },\n",
    "            'operators': {\n",
    "                'unzip-file':{\n",
    "                    'operator_in_dir': DATA_DIR\n",
    "                },\n",
    "                'openmined-node': {\n",
    "                    'global.id': identifier,\n",
    "                    'port': 5000,\n",
    "                    'grid_network_url': GRID_NETWORK_URL\n",
    "                },\n",
    "                'data-provider': {\n",
    "                    'dataset': DATASET,\n",
    "                    'lifespan': 60 * 23, #60 * n_hours\n",
    "                    'exp_tag': EXP_TAG\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }   \n",
    "    url = f'https://{machine}/flow/kaapana/api/trigger/openmined-provide-data'\n",
    "    print(url)\n",
    "    \n",
    "    r = requests.post(url, json=json_data, verify=False)\n",
    "    print(r.json())\n",
    "\n",
    "# Timestamp    \n",
    "ts_trigger = time.time()\n",
    "ts_trigger_date = datetime.fromtimestamp(ts_trigger).strftime('%Y-%b-%d-%H-%M-%S')\n",
    "timestamps.append({\n",
    "    'description': 'trigger_dags',\n",
    "    'epoch': '',\n",
    "    'worker': '',\n",
    "    'ts': ts_trigger,\n",
    "    'ts_date': ts_trigger_date\n",
    "})\n",
    "logging.info('Triggered Openmined DAGs on workers:\\t{}'.format(ts_trigger_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the nodes are setup and the data-providing operators prepared the data, you can find it by searching the gri network. If the results are empty, check the remote machines before proceeding (still, it might need some time until it's up and running):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for targets in grid\n",
    "grid.search('#Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for images in grid\n",
    "grid.search('#X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset implemented to work with the pointers to the remote data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OpenminedDataset(Dataset):\n",
    "    '''Openmined Dataset using pointers to remote data instances'''\n",
    "    \n",
    "    def __init__(self, img_ptr, label_ptr):\n",
    "        self.img_ptr = img_ptr\n",
    "        self.label_ptr = label_ptr\n",
    "        self.transform = None #transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ptr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Return image and corresponding label'''\n",
    "        img_ptr = self.img_ptr[idx]\n",
    "        label_ptr = self.label_ptr[idx]\n",
    "        \n",
    "        return img_ptr, label_ptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data references and prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search data\n",
    "print('Data:')\n",
    "print(grid.search(\"#X\", \"#xray\", \"#dataset\", \"#xray-exp\"))\n",
    "data = grid.search(\"#X\", \"#xray\", \"#dataset\", \"#xray-exp\")\n",
    "\n",
    "print('\\nLabel:')\n",
    "print(grid.search(\"#Y\", \"#xray\", \"#dataset\", \"#xray-exp\"))\n",
    "labels = grid.search(\"#Y\", \"#xray\", \"#dataset\", \"#xray-exp\")\n",
    "\n",
    "# get workers and their locations\n",
    "workers = {worker : data[worker][0].location for worker in data.keys()}\n",
    "print('\\nWorkers:')\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders using the pointers-datasets\n",
    "dataloaders = dict()\n",
    "\n",
    "for worker in workers.items():\n",
    "    name = worker[0]\n",
    "    dataset = OpenminedDataset(data[name][0],labels[name][0])\n",
    "    \n",
    "    dataloaders[name] = DataLoader(\n",
    "        dataset,\n",
    "        batch_size= args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "model = ResNet18()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "# initil timstamp\n",
    "ts_initial = time.time()\n",
    "timestamps.append({\n",
    "    'description': 'initial',\n",
    "    'epoch': '',\n",
    "    'worker': '',\n",
    "    'ts': ts_initial,\n",
    "    'ts_date': datetime.fromtimestamp(ts_initial).strftime('%Y-%b-%d-%H-%M-%S')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "\n",
    "print('\\n##### RUN MODEL TRAINING #####')\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print('# EPOCH: {}'.format(epoch))\n",
    "    \n",
    "    # iterate over the remote workers - send model to its location\n",
    "    for identifier, worker in workers.items():\n",
    "        model.train()\n",
    "        model.send(worker)\n",
    "        loss_acc = 0\n",
    "    \n",
    "        # iterate over batches of remote data on current worker\n",
    "        for batch_idx, (imgs, labels) in enumerate(dataloaders[worker.id]):\n",
    "            pred = model(imgs)\n",
    "            loss = F.nll_loss(pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_value = loss.get()\n",
    "            loss_acc += loss_value\n",
    "            \n",
    "        # get model and calc avg loss from worker\n",
    "        model.get()\n",
    "        loss_avg = loss_acc.item() / len(dataloaders[worker.id].dataset)\n",
    "        print('Train epoch: {} | Worker: {} | Loss: {:.6}'.format(epoch, worker.id, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "torch.save(model.state_dict(), './model_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model performance on test data\n",
    "\n",
    "After training is finished, the models performance is tested on the Pneunoia test data (made available in .\\data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet values\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_data_dir = './data/test'\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=datasets.ImageFolder(root=test_data_dir, transform=img_transforms),\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "print('Images:', len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    targets, predictions = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            targets.extend(target.tolist())\n",
    "            predictions.extend([item[0] for item in pred.tolist()])\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = ResNet18()\n",
    "model.load_state_dict(torch.load('./model_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run testing\n",
    "test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

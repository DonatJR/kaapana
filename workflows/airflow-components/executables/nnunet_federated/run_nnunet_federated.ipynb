{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import torch\n",
    "import collections\n",
    "from nnunet.training.model_restore import restore_model\n",
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '/executables')\n",
    "from kaapana_federated.kaapana_federated import KaapanaFederatedTrainingBase, requests_retry_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dkfz', 'hamburg']\n"
     ]
    }
   ],
   "source": [
    "class nnUNetFederatedTraining(KaapanaFederatedTrainingBase):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_network_trainer(folder):\n",
    "        checkpoint = join(folder, \"model_final_checkpoint.model\")\n",
    "        pkl_file = checkpoint + \".pkl\"\n",
    "        return restore_model(pkl_file, checkpoint, False)\n",
    "\n",
    "    def __init__(self, run_id=None, workflow_dir=None, federated_operators=None, skip_operators=None):\n",
    "        dag_id = 'nnunet-training'\n",
    "        run_id = run_id or os.getenv(\"RUN_ID\", dag_id + str(uuid.uuid4()))\n",
    "        workflow_dir = workflow_dir or os.getenv('WORKFLOW_DIR', f'/appdata/dev/federated-local-workspace/{run_id}')\n",
    "        federated_operators = federated_operators or ['nnunet-training']\n",
    "        skip_operators = skip_operators or [\"zip-unzip-training\", \"model2dicom\", \"dcmsend\", \"pdf2dcm-training\", \"dcmsend-pdf\", \"workflow-cleaner\"]\n",
    "        conf_data = KaapanaFederatedTrainingBase.get_conf(dag_id, run_id, workflow_dir, federated_operators, skip_operators)\n",
    "        \n",
    "        super().__init__(dag_id, conf_data, workflow_dir)\n",
    "        \n",
    "        if self.remote_conf_data['workflow_form']['train_max_epochs'] % self.remote_conf_data['federated_form']['federated_total_rounds'] != 0:\n",
    "            raise ValueError('train_max_epochs has to be multiple of federated_total_rounds')\n",
    "        else:\n",
    "            self.remote_conf_data['workflow_form']['epochs_per_round'] = int(self.remote_conf_data['workflow_form']['train_max_epochs'] / self.remote_conf_data['federated_form']['federated_total_rounds'])\n",
    "\n",
    "        print(f\"Epochs per round {self.remote_conf_data['workflow_form']['epochs_per_round']}\")\n",
    "        \n",
    "    def update_data(self, tmp_federated_site_info, federated_round):     \n",
    "        print(Path(os.path.join(self.fl_working_dir, str(federated_round))))\n",
    "        models_path = Path(os.path.join(self.fl_working_dir, str(federated_round)))\n",
    "        averaged_state_dict = collections.OrderedDict()\n",
    "        averaged_amp_grad_scaler = dict()\n",
    "        print('Loading averaged checkpoints')\n",
    "        for idx, fname in enumerate(models_path.rglob('model_final_checkpoint.model')):\n",
    "            print(fname)\n",
    "            checkpoint = torch.load(fname, map_location=torch.device('cpu'))\n",
    "            if idx==0:\n",
    "                for key, value in checkpoint['state_dict'].items():\n",
    "                    averaged_state_dict[key] = value\n",
    "                if 'amp_grad_scaler' in checkpoint.keys():\n",
    "                    for key, value in checkpoint['amp_grad_scaler'].items():\n",
    "                        averaged_amp_grad_scaler[key] = value \n",
    "            else:\n",
    "                for key, value in checkpoint['state_dict'].items():\n",
    "                    averaged_state_dict[key] =  (averaged_state_dict[key] + checkpoint['state_dict'][key]) / 2.\n",
    "                if 'amp_grad_scaler' in checkpoint.keys():\n",
    "                    for key, value in checkpoint['amp_grad_scaler'].items():\n",
    "                        averaged_amp_grad_scaler[key] = (averaged_amp_grad_scaler[key] + checkpoint['amp_grad_scaler'][key]) / 2.\n",
    "\n",
    "        print('Saving averaged checkpoints')\n",
    "        for idx, fname in enumerate(models_path.rglob('model_final_checkpoint.model')):\n",
    "            print(fname)\n",
    "            checkpoint['state_dict'] = averaged_state_dict\n",
    "            if 'amp_grad_scaler' in checkpoint.keys():\n",
    "                checkpoint['amp_grad_scaler'] = averaged_amp_grad_scaler\n",
    "            torch.save(checkpoint, fname)\n",
    "\n",
    "        self.remote_conf_data['workflow_form']['train_continue'] = True\n",
    "        print(federated_round, self.remote_conf_data['federated_form']['federated_total_rounds'])\n",
    "            \n",
    "kaapana_ft = nnUNetFederatedTraining()\n",
    "kaapana_ft.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
